#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""æ³•å¾‹æ¡æ¬¾å¤šç»´åº¦æ ‡ç­¾ä½“ç³»è‡ªåŠ¨æ ‡æ³¨ç³»ç»Ÿ

åŠŸèƒ½æ¦‚è¿°ï¼š
è¯¥ç³»ç»Ÿç”¨äºæ„å»ºå¤šç»´åº¦æ ‡ç­¾ä½“ç³»ï¼Œé€šè¿‡åŠç›‘ç£å­¦ä¹ æ–¹æ³•å¯¹æ³•å¾‹æ¡æ¬¾è¿›è¡Œè‡ªåŠ¨åˆ†ç±»æ ‡æ³¨ï¼Œ
æ”¯æŒ"æ³•è§„ç±»å‹"ã€"æ•ˆåŠ›ç­‰çº§"ã€"ç®¡è¾–åŒºåŸŸ"ç­‰å¤šä¸ªç»´åº¦çš„æ ‡ç­¾ä½“ç³»ï¼Œå¯å¤„ç†ç™¾ä¸‡çº§æ¡æ¬¾æ•°æ®ã€‚

ä¸»è¦åŠŸèƒ½ï¼š
- å®šä¹‰å¤šç»´åº¦æ ‡ç­¾æ ‘ç»“æ„
- å®ç°åŸºäºè§„åˆ™çš„åˆå§‹æ ‡ç­¾æ ‡æ³¨
- æ„å»ºåŠç›‘ç£å­¦ä¹ æ¨¡å‹è¿›è¡Œè‡ªåŠ¨åˆ†ç±»
- æ”¯æŒç™¾ä¸‡çº§æ¡æ¬¾çš„æ‰¹é‡å¤„ç†
- ä¸ç°æœ‰å“ˆå¸Œå€¼å’Œå…ƒæ•°æ®ç³»ç»Ÿé›†æˆ
- æä¾›æ ‡æ³¨ç»“æœå¯è§†åŒ–å’Œè¯„ä¼°åŠŸèƒ½

ä½¿ç”¨æ–¹æ³•ï¼š
1. å‡†å¤‡å·²åˆ†å‰²çš„æ³•å¾‹æ¡æ¬¾JSONæ–‡ä»¶
2. é…ç½®æ ‡ç­¾ä½“ç³»å’Œæ¨¡å‹å‚æ•°
3. è¿è¡Œç¨‹åºè¿›è¡Œè‡ªåŠ¨æ ‡æ³¨
4. æŸ¥çœ‹å’Œå¯¼å‡ºæ ‡æ³¨ç»“æœ

è¾“å…¥è¾“å‡ºï¼š
è¾“å…¥ï¼šå·²åˆ†å‰²çš„æ³•å¾‹æ¡æ¬¾JSONæ–‡ä»¶ï¼ˆå¦‚'ä¸­åäººæ°‘å…±å’Œå›½æ°‘æ³•å…¸_æ¡æ¬¾åˆ†å‰².json'ï¼‰
è¾“å‡ºï¼š
  - å¸¦å¤šç»´åº¦æ ‡ç­¾çš„æ³•å¾‹æ¡æ¬¾JSONï¼ˆå¦‚'ä¸­åäººæ°‘å…±å’Œå›½æ°‘æ³•å…¸_æ ‡ç­¾æ ‡æ³¨.json'ï¼‰
  - æ ‡ç­¾ç»Ÿè®¡å’Œè¯„ä¼°æŠ¥å‘Š
"""

import os
# éšè— TensorFlow æ—¥å¿—ä¿¡æ¯
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # éšè— INFO å’Œ WARNING æ—¥å¿—
import warnings
# å±è”½ matplotlib å­—ä½“è­¦å‘Š
warnings.filterwarnings('ignore', message='.*Font family.*not found.*')
warnings.filterwarnings('ignore', module='matplotlib.font_manager')
warnings.filterwarnings('ignore', category=UserWarning, message='.*findfont.*')
warnings.filterwarnings('ignore', category=UserWarning, message='.*Font family.*')

# è¿›ä¸€æ­¥å±è”½ Matplotlib çš„ findfont æ—¥å¿—
import logging
import matplotlib as mpl
mpl.set_loglevel('error')
logging.getLogger('matplotlib').setLevel(logging.ERROR)
logging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR)
import re
import json
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Embedding, LSTM, Bidirectional, Dropout, Input
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from sklearn.semi_supervised import SelfTrainingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import classification_report
import matplotlib.pyplot as plt
import hashlib
import time
from tqdm import tqdm
import concurrent.futures
from typing import List, Dict, Tuple, Set, Any

# è·å–è„šæœ¬æ‰€åœ¨ç›®å½•
script_dir = os.path.dirname(os.path.abspath(__file__))

# ç¡®ä¿ä¸­æ–‡æ˜¾ç¤ºæ­£å¸¸
plt.rcParams["font.family"] = ["SimHei", "WenQuanYi Micro Hei", "Heiti TC"]

class LegalMultiDimensionalTagger:
    """æ³•å¾‹æ¡æ¬¾å¤šç»´åº¦æ ‡ç­¾è‡ªåŠ¨æ ‡æ³¨ç³»ç»Ÿ"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ ‡ç­¾ç³»ç»Ÿï¼Œå®šä¹‰å¤šç»´åº¦æ ‡ç­¾æ ‘"""
        # è·å–è„šæœ¬æ‰€åœ¨ç›®å½•
        self.script_dir = os.path.dirname(os.path.abspath(__file__))
        
        # å¤šç»´åº¦æ ‡ç­¾ä½“ç³»å®šä¹‰
        self.tag_trees = {
            "æ³•è§„ç±»å‹": {
                "åŸºæœ¬æ³•": ["å®ªæ³•", "æ°‘æ³•å…¸", "åˆ‘æ³•å…¸"],
                "å•è¡Œæ³•": ["åŠ³åŠ¨æ³•", "åˆåŒæ³•", "ç‰©æƒæ³•", "ä¾µæƒè´£ä»»æ³•", "çŸ¥è¯†äº§æƒæ³•"],
                "è¡Œæ”¿æ³•è§„": ["æ¡ä¾‹", "è§„å®š", "åŠæ³•", "ç»†åˆ™"],
                "åœ°æ–¹æ³•è§„": [],
                "å¸æ³•è§£é‡Š": [],
                "éƒ¨é—¨è§„ç« ": [],
                "å›½é™…æ¡çº¦": []
            },
            "æ•ˆåŠ›ç­‰çº§": {
                "å®ªæ³•": [],
                "æ³•å¾‹": [],
                "è¡Œæ”¿æ³•è§„": [],
                "åœ°æ–¹æ³•è§„": [],
                "éƒ¨é—¨è§„ç« ": [],
                "è§„èŒƒæ€§æ–‡ä»¶": []
            },
            "ç®¡è¾–åŒºåŸŸ": {
                "å…¨å›½æ€§": ["ä¸­å¤®", "å…¨å›½"],
                "åœ°æ–¹": {
                    "ååŒ—": ["åŒ—äº¬", "å¤©æ´¥", "æ²³åŒ—", "å±±è¥¿", "å†…è’™å¤"],
                    "ä¸œåŒ—": ["è¾½å®", "å‰æ—", "é»‘é¾™æ±Ÿ"],
                    "åä¸œ": ["ä¸Šæµ·", "æ±Ÿè‹", "æµ™æ±Ÿ", "å®‰å¾½", "ç¦å»º", "æ±Ÿè¥¿", "å±±ä¸œ"],
                    "åå—": ["å¹¿ä¸œ", "å¹¿è¥¿", "æµ·å—"],
                    "åä¸­": ["æ²³å—", "æ¹–åŒ—", "æ¹–å—"],
                    "è¥¿å—": ["é‡åº†", "å››å·", "è´µå·", "äº‘å—", "è¥¿è—"],
                    "è¥¿åŒ—": ["é™•è¥¿", "ç”˜è‚ƒ", "é’æµ·", "å®å¤", "æ–°ç–†"],
                    "æ¸¯æ¾³å°": ["é¦™æ¸¯", "æ¾³é—¨", "å°æ¹¾"]
                }
            },
            "å†…å®¹é¢†åŸŸ": {
                "æ°‘äº‹": ["åˆåŒ", "ç‰©æƒ", "ä¾µæƒ", "å©šå§»å®¶åº­", "ç»§æ‰¿"],
                "åˆ‘äº‹": ["çŠ¯ç½ª", "åˆ‘ç½š", "åˆ‘äº‹è¯‰è®¼"],
                "è¡Œæ”¿": ["è¡Œæ”¿è®¸å¯", "è¡Œæ”¿å¤„ç½š", "è¡Œæ”¿å¼ºåˆ¶", "è¡Œæ”¿å¤è®®"],
                "ç»æµ": ["å…¬å¸", "é‡‘è", "ç¨æ”¶", "çŸ¥è¯†äº§æƒ"],
                "åŠ³åŠ¨": ["åŠ³åŠ¨åˆåŒ", "å·¥èµ„ç¦åˆ©", "åŠ³åŠ¨äº‰è®®"],
                "ç¯å¢ƒ": [],
                "è¯‰è®¼": ["æ°‘äº‹è¯‰è®¼", "åˆ‘äº‹è¯‰è®¼", "è¡Œæ”¿è¯‰è®¼"]
            }
        }
        
        # æ ‡ç­¾è§„åˆ™åº“
        self.tag_rules = {
            "æ³•è§„ç±»å‹": {
                "å®ªæ³•": [r'å®ªæ³•'],
                "æ°‘æ³•å…¸": [r'æ°‘æ³•å…¸'],
                "åˆ‘æ³•å…¸": [r'åˆ‘æ³•å…¸'],
                "åŠ³åŠ¨æ³•": [r'åŠ³åŠ¨æ³•'],
                "åˆåŒæ³•": [r'åˆåŒæ³•'],
                "ç‰©æƒæ³•": [r'ç‰©æƒæ³•'],
                "æ¡ä¾‹": [r'æ¡ä¾‹'],
                "è§„å®š": [r'è§„å®š'],
                "åŠæ³•": [r'åŠæ³•'],
                "ç»†åˆ™": [r'ç»†åˆ™']
            },
            "æ•ˆåŠ›ç­‰çº§": {
                "å®ªæ³•": [r'å®ªæ³•'],
                "æ³•å¾‹": [r'æ³•$'],
                "è¡Œæ”¿æ³•è§„": [r'æ¡ä¾‹'],
                "åœ°æ–¹æ³•è§„": [r'([çœå¸‚è‡ªæ²»åŒº]|[äº¬æ´¥æ²ªæ¸])[^ï¼Œ,]+æ¡ä¾‹'],
                "éƒ¨é—¨è§„ç« ": [r'è§„å®š|åŠæ³•|ç»†åˆ™'],
                "è§„èŒƒæ€§æ–‡ä»¶": [r'é€šçŸ¥|æ„è§|å†³å®š']
            },
            "ç®¡è¾–åŒºåŸŸ": {
                "å…¨å›½æ€§": [r'ä¸­åäººæ°‘å…±å’Œå›½', r'å…¨å›½', r'ä¸­å¤®'],
                "åŒ—äº¬": [r'åŒ—äº¬å¸‚'],
                "ä¸Šæµ·": [r'ä¸Šæµ·å¸‚'],
                "å¹¿ä¸œ": [r'å¹¿ä¸œçœ'],
                "æ±Ÿè‹": [r'æ±Ÿè‹çœ'],
                # å…¶ä»–åœ°åŒºè§„åˆ™...
            },
            "å†…å®¹é¢†åŸŸ": {
                "åˆåŒ": [r'åˆåŒ|åè®®|å¥‘çº¦'],
                "ç‰©æƒ": [r'æ‰€æœ‰æƒ|ç”¨ç›Šç‰©æƒ|æ‹…ä¿ç‰©æƒ|ä¸åŠ¨äº§|åŠ¨äº§'],
                "ä¾µæƒ": [r'ä¾µæƒ|æŸå®³èµ”å¿|è¿‡é”™è´£ä»»'],
                "å©šå§»å®¶åº­": [r'ç»“å©š|ç¦»å©š|å¤«å¦»|å­å¥³|å®¶åº­'],
                "ç»§æ‰¿": [r'ç»§æ‰¿|é—å˜±|é—äº§'],
                "å…¬å¸": [r'å…¬å¸|ä¼ä¸š|è‚¡ä¸œ|è‘£äº‹'],
                "åŠ³åŠ¨åˆåŒ": [r'åŠ³åŠ¨åˆåŒ|é›‡ä½£|åŠ³åŠ¨è€…|ç”¨äººå•ä½'],
                "çŸ¥è¯†äº§æƒ": [r'å•†æ ‡|ä¸“åˆ©|è‘—ä½œæƒ|çŸ¥è¯†äº§æƒ']
            }
        }
        
        # ç¼–è¯‘è§„åˆ™çš„æ­£åˆ™è¡¨è¾¾å¼ä»¥æé«˜æ€§èƒ½
        self.compiled_rules = {}
        for dimension, tags in self.tag_rules.items():
            self.compiled_rules[dimension] = {}
            for tag_name, patterns in tags.items():
                self.compiled_rules[dimension][tag_name] = [re.compile(pattern) for pattern in patterns]
        
        # æ¨¡å‹å‚æ•°
        self.max_seq_length = 512
        self.embedding_dim = 256
        self.lstm_units = 128
        self.batch_size = 32
        self.epochs = 10
        
        # æ•°æ®å­˜å‚¨
        self.articles = []
        self.tagged_articles = []
        self.metadata_store = {}
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.models = {}
        self.tokenizer = Tokenizer(num_words=50000, oov_token="<UNK>")
        
        # å¹¶è¡Œå¤„ç†å‚æ•°
        self.max_workers = min(10, os.cpu_count() or 4)
        
    def _generate_sha256(self, text: str) -> str:
        """ä¸ºæ–‡æœ¬ç”ŸæˆSHA-256å“ˆå¸Œå€¼"""
        return hashlib.sha256(text.encode('utf-8')).hexdigest()
    
    def load_articles(self, file_path: str, source_institution: str = "æœªçŸ¥æœºæ„", 
                      revision_history: List[Dict] = None) -> List[Dict]:
        """åŠ è½½å·²åˆ†å‰²çš„æ³•å¾‹æ¡æ¬¾æ–‡ä»¶"""
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        print(f"æ­£åœ¨åŠ è½½æ¡æ¬¾æ–‡ä»¶: {file_path}")
        with open(file_path, 'r', encoding='utf-8') as f:
            self.articles = json.load(f)
        
        # æ·»åŠ å“ˆå¸Œå€¼å’Œå…ƒæ•°æ®
        for article in self.articles:
            article_id = article.get('id', '')
            text = article.get('text', '')
            
            # ç”ŸæˆSHA-256å“ˆå¸Œå€¼
            text_hash = self._generate_sha256(text)
            article['hash'] = text_hash
            
            # æ„å»ºå…ƒæ•°æ®
            metadata = {
                'hash': text_hash,
                'source_institution': source_institution,
                'revision_history': revision_history or [{
                    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
                    'author': "ç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆ",
                    'comment': "åˆå§‹å¯¼å…¥"
                }],
                'source_file': os.path.basename(file_path),
                'import_time': time.strftime('%Y-%m-%d %H:%M:%S'),
                'content_checksum': text_hash
            }
            
            # å­˜å‚¨å…ƒæ•°æ®
            self.metadata_store[article_id] = metadata
        
        print(f"æˆåŠŸåŠ è½½ {len(self.articles)} æ¡æ³•å¾‹æ¡æ¬¾")
        return self.articles
    
    def rule_based_tagging(self) -> List[Dict]:
        """ä½¿ç”¨åŸºäºè§„åˆ™çš„æ–¹æ³•è¿›è¡Œåˆå§‹æ ‡ç­¾æ ‡æ³¨"""
        print("æ­£åœ¨è¿›è¡ŒåŸºäºè§„åˆ™çš„åˆå§‹æ ‡ç­¾æ ‡æ³¨...")
        
        # åˆ›å»ºä¸€ä¸ªæ–°çš„åˆ—è¡¨å­˜å‚¨å¸¦æ ‡ç­¾çš„æ¡æ¬¾
        self.tagged_articles = []
        
        for article in tqdm(self.articles, desc="è§„åˆ™æ ‡æ³¨è¿›åº¦"):
            tagged_article = article.copy()
            tags = {dimension: set() for dimension in self.tag_trees.keys()}
            
            text = article.get('text', '').lower()
            title = article.get('title', '').lower() if 'title' in article else ''
            full_text = text + " " + title
            
            # å¯¹æ¯ä¸ªç»´åº¦åº”ç”¨è§„åˆ™
            for dimension, dimension_rules in self.compiled_rules.items():
                for tag_name, patterns in dimension_rules.items():
                    for pattern in patterns:
                        if pattern.search(full_text):
                            tags[dimension].add(tag_name)
                            
                            # æ·»åŠ çˆ¶æ ‡ç­¾ï¼ˆå¦‚æœæœ‰ï¼‰
                            parent_tags = self._get_parent_tags(dimension, tag_name)
                            tags[dimension].update(parent_tags)
            
            # è½¬æ¢é›†åˆä¸ºåˆ—è¡¨
            for dim in tags:
                tags[dim] = list(tags[dim])
            
            tagged_article['tags'] = tags
            tagged_article['tag_source'] = 'rule-based'
            self.tagged_articles.append(tagged_article)
        
        print("åŸºäºè§„åˆ™çš„æ ‡ç­¾æ ‡æ³¨å®Œæˆ")
        return self.tagged_articles
    
    def _get_parent_tags(self, dimension: str, tag_name: str) -> Set[str]:
        """è·å–æ ‡ç­¾çš„çˆ¶æ ‡ç­¾"""
        parent_tags = set()
        
        # ç®€å•çš„çˆ¶æ ‡ç­¾æŸ¥æ‰¾é€»è¾‘ï¼Œå®é™…åº”ç”¨ä¸­å¯èƒ½éœ€è¦æ›´å¤æ‚çš„æ ‘éå†
        dimension_tree = self.tag_trees.get(dimension, {})
        
        for parent, children in dimension_tree.items():
            if isinstance(children, list) and tag_name in children:
                parent_tags.add(parent)
            elif isinstance(children, dict):
                for _, sub_children in children.items():
                    if tag_name in sub_children:
                        parent_tags.add(parent)
        
        return parent_tags
    
    def prepare_training_data(self) -> Tuple[np.ndarray, Dict[str, np.ndarray]]:
        """å‡†å¤‡åŠç›‘ç£å­¦ä¹ çš„è®­ç»ƒæ•°æ®"""
        print("æ­£åœ¨å‡†å¤‡åŠç›‘ç£å­¦ä¹ çš„è®­ç»ƒæ•°æ®...")
        
        # æå–æ–‡æœ¬å’Œæ ‡ç­¾
        texts = [article.get('text', '') for article in self.tagged_articles]
        
        # ä¸ºæ¯ä¸ªç»´åº¦åˆ›å»ºæ ‡ç­¾çŸ©é˜µ
        dimension_labels = {}
        for dimension in self.tag_trees.keys():
            # è·å–æ‰€æœ‰å¯èƒ½çš„æ ‡ç­¾
            all_tags = self._get_all_tags_in_dimension(dimension)
            tag_to_idx = {tag: i for i, tag in enumerate(all_tags)}
            
            # åˆ›å»ºæ ‡ç­¾çŸ©é˜µ
            labels_matrix = np.zeros((len(texts), len(all_tags)))
            for i, article in enumerate(self.tagged_articles):
                article_tags = article.get('tags', {}).get(dimension, [])
                for tag in article_tags:
                    if tag in tag_to_idx:
                        labels_matrix[i, tag_to_idx[tag]] = 1
            
            dimension_labels[dimension] = labels_matrix
        
        # è®­ç»ƒtokenizer
        self.tokenizer.fit_on_texts(texts)
        
        # è½¬æ¢æ–‡æœ¬ä¸ºåºåˆ—
        sequences = self.tokenizer.texts_to_sequences(texts)
        padded_sequences = pad_sequences(sequences, maxlen=self.max_seq_length, padding='post', truncating='post')
        
        print("è®­ç»ƒæ•°æ®å‡†å¤‡å®Œæˆ")
        return padded_sequences, dimension_labels
    
    def _get_all_tags_in_dimension(self, dimension: str) -> List[str]:
        """è·å–æŸä¸ªç»´åº¦ä¸‹çš„æ‰€æœ‰æ ‡ç­¾"""
        all_tags = []
        dimension_tree = self.tag_trees.get(dimension, {})
        
        def traverse_tree(tree_node):
            if isinstance(tree_node, dict):
                for key, value in tree_node.items():
                    all_tags.append(key)
                    traverse_tree(value)
            elif isinstance(tree_node, list):
                all_tags.extend(tree_node)
        
        traverse_tree(dimension_tree)
        return list(set(all_tags))  # å»é‡
    
    def build_model(self, dimension: str) -> Model:
        """ä¸ºç‰¹å®šç»´åº¦æ„å»ºåŠç›‘ç£å­¦ä¹ æ¨¡å‹"""
        print(f"æ­£åœ¨æ„å»º {dimension} ç»´åº¦çš„æ ‡ç­¾æ¨¡å‹...")
        
        # è·å–è¯¥ç»´åº¦çš„æ ‡ç­¾æ•°é‡
        all_tags = self._get_all_tags_in_dimension(dimension)
        num_classes = len(all_tags)
        
        # æ„å»ºæ¨¡å‹
        inputs = Input(shape=(self.max_seq_length,))
        embedding = Embedding(input_dim=min(len(self.tokenizer.word_index) + 1, 50000),
                              output_dim=self.embedding_dim)(inputs)
        
        # åŒå‘LSTMå±‚
        x = Bidirectional(LSTM(self.lstm_units, return_sequences=True))(embedding)
        x = Dropout(0.3)(x)
        x = Bidirectional(LSTM(self.lstm_units))(x)
        x = Dropout(0.3)(x)
        
        # è¾“å‡ºå±‚ï¼ˆå¤šæ ‡ç­¾åˆ†ç±»ï¼‰
        outputs = Dense(num_classes, activation='sigmoid')(x)
        
        model = Model(inputs=inputs, outputs=outputs)
        model.compile(optimizer='adam',
                     loss='binary_crossentropy',
                     metrics=['accuracy'])
        
        print(f"{dimension} ç»´åº¦æ¨¡å‹æ„å»ºå®Œæˆ")
        return model
    
    def train_models(self, x_train: np.ndarray, y_train_dict: Dict[str, np.ndarray]) -> Dict[str, Model]:
        """è®­ç»ƒæ‰€æœ‰ç»´åº¦çš„æ ‡ç­¾æ¨¡å‹"""
        print("å¼€å§‹è®­ç»ƒæ‰€æœ‰ç»´åº¦çš„æ ‡ç­¾æ¨¡å‹...")
        
        # ä¸ºæ¯ä¸ªç»´åº¦è®­ç»ƒä¸€ä¸ªæ¨¡å‹
        for dimension, y_train in y_train_dict.items():
            print(f"è®­ç»ƒ {dimension} ç»´åº¦æ¨¡å‹...")
            
            # æ„å»ºæ¨¡å‹
            model = self.build_model(dimension)
            
            # åˆ›å»ºå›è°ƒ
            early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
            model_checkpoint = ModelCheckpoint(os.path.join(self.script_dir, f'model_{dimension}.keras'), save_best_only=True)
            
            # è®­ç»ƒæ¨¡å‹
            model.fit(x_train, y_train,
                     batch_size=self.batch_size,
                     epochs=self.epochs,
                     validation_split=0.2,
                     callbacks=[early_stopping, model_checkpoint],
                     verbose=1)
            
            # ä¿å­˜æ¨¡å‹
            self.models[dimension] = model
        
        print("æ‰€æœ‰ç»´åº¦æ¨¡å‹è®­ç»ƒå®Œæˆ")
        return self.models
    
    def semi_supervised_tagging(self, confidence_threshold: float = 0.8) -> List[Dict]:
        """ä½¿ç”¨åŠç›‘ç£å­¦ä¹ æ–¹æ³•è¿›è¡Œæ ‡ç­¾æ ‡æ³¨"""
        print("æ­£åœ¨è¿›è¡ŒåŠç›‘ç£å­¦ä¹ æ ‡ç­¾æ ‡æ³¨...")
        
        # å‡†å¤‡è®­ç»ƒæ•°æ®
        x_train, y_train_dict = self.prepare_training_data()
        
        # è®­ç»ƒåˆå§‹æ¨¡å‹
        self.train_models(x_train, y_train_dict)
        
        # ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹
        updated_articles = []
        for article in tqdm(self.tagged_articles, desc="åŠç›‘ç£æ ‡æ³¨è¿›åº¦"):
            updated_article = article.copy()
            text = article.get('text', '')
            
            # è½¬æ¢æ–‡æœ¬ä¸ºåºåˆ—
            sequence = self.tokenizer.texts_to_sequences([text])[0]
            padded_sequence = pad_sequences([sequence], maxlen=self.max_seq_length, padding='post', truncating='post')
            
            # å¯¹æ¯ä¸ªç»´åº¦è¿›è¡Œé¢„æµ‹
            for dimension, model in self.models.items():
                # è·å–æ‰€æœ‰å¯èƒ½çš„æ ‡ç­¾
                all_tags = self._get_all_tags_in_dimension(dimension)
                
                # é¢„æµ‹
                predictions = model.predict(padded_sequence)[0]
                
                # åº”ç”¨ç½®ä¿¡åº¦é˜ˆå€¼
                predicted_tags = [all_tags[i] for i, prob in enumerate(predictions) if prob >= confidence_threshold]
                
                # å¦‚æœåŠç›‘ç£é¢„æµ‹ç»“æœä¸ä¸ºç©ºï¼Œåˆ™æ›´æ–°æ ‡ç­¾
                if predicted_tags:
                    # åˆå¹¶è§„åˆ™æ ‡ç­¾å’ŒåŠç›‘ç£æ ‡ç­¾
                    rule_tags = set(article.get('tags', {}).get(dimension, []))
                    combined_tags = list(rule_tags.union(set(predicted_tags)))
                    updated_article['tags'][dimension] = combined_tags
            
            updated_article['tag_source'] = 'semi-supervised'
            updated_articles.append(updated_article)
        
        self.tagged_articles = updated_articles
        print("åŠç›‘ç£å­¦ä¹ æ ‡ç­¾æ ‡æ³¨å®Œæˆ")
        return self.tagged_articles
    
    def batch_process_articles(self, file_paths: List[str], 
                             source_institution: str = "æœªçŸ¥æœºæ„") -> Dict[str, List[Dict]]:
        """æ‰¹é‡å¤„ç†å¤šä¸ªæ¡æ¬¾æ–‡ä»¶"""
        print(f"å¼€å§‹æ‰¹é‡å¤„ç† {len(file_paths)} ä¸ªæ–‡ä»¶...")
        
        results = {}
        
        for file_path in tqdm(file_paths, desc="æ–‡ä»¶å¤„ç†è¿›åº¦"):
            try:
                # åŠ è½½æ–‡ä»¶
                self.load_articles(file_path, source_institution)
                
                # è§„åˆ™æ ‡æ³¨
                self.rule_based_tagging()
                
                # åŠç›‘ç£æ ‡æ³¨
                self.semi_supervised_tagging()
                
                # ä¿å­˜ç»“æœ
                results[file_path] = self.tagged_articles.copy()
                
                # å¯¼å‡ºç»“æœ
                base_name = os.path.splitext(os.path.basename(file_path))[0] + "_æ ‡ç­¾æ ‡æ³¨.json"
                output_file = os.path.join(self.script_dir, base_name)
                self.export_tagged_articles(output_file)
                
            except Exception as e:
                print(f"å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {str(e)}")
                results[file_path] = []
        
        print("æ‰¹é‡å¤„ç†å®Œæˆ")
        return results
    
    def parallel_batch_process(self, file_paths: List[str], 
                             source_institution: str = "æœªçŸ¥æœºæ„") -> Dict[str, List[Dict]]:
        """å¹¶è¡Œæ‰¹é‡å¤„ç†å¤šä¸ªæ¡æ¬¾æ–‡ä»¶ï¼Œé€‚ç”¨äºç™¾ä¸‡çº§æ•°æ®"""
        print(f"å¼€å§‹å¹¶è¡Œæ‰¹é‡å¤„ç† {len(file_paths)} ä¸ªæ–‡ä»¶...")
        
        results = {}
        
        def process_single_file(file_path):
            try:
                # åˆ›å»ºæ–°çš„å®ä¾‹è¿›è¡Œå¤„ç†ï¼Œé¿å…çº¿ç¨‹å®‰å…¨é—®é¢˜
                tagger = LegalMultiDimensionalTagger()
                
                # åŠ è½½æ–‡ä»¶
                tagger.load_articles(file_path, source_institution)
                
                # è§„åˆ™æ ‡æ³¨
                tagger.rule_based_tagging()
                
                # åŠç›‘ç£æ ‡æ³¨
                tagger.semi_supervised_tagging()
                
                # å¯¼å‡ºç»“æœ
                base_name = os.path.splitext(os.path.basename(file_path))[0] + "_æ ‡ç­¾æ ‡æ³¨.json"
                output_file = os.path.join(tagger.script_dir, base_name)
                tagger.export_tagged_articles(output_file)
                
                return file_path, tagger.tagged_articles
            except Exception as e:
                print(f"å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {str(e)}")
                return file_path, []
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_file = {executor.submit(process_single_file, file_path): file_path for file_path in file_paths}
            
            # æ”¶é›†ç»“æœ
            for future in tqdm(concurrent.futures.as_completed(future_to_file), total=len(file_paths), desc="å¹¶è¡Œå¤„ç†è¿›åº¦"):
                file_path = future_to_file[future]
                try:
                    file_path, articles = future.result()
                    results[file_path] = articles
                except Exception as e:
                    print(f"è·å–æ–‡ä»¶ {file_path} ç»“æœæ—¶å‡ºé”™: {str(e)}")
                    results[file_path] = []
        
        print("å¹¶è¡Œæ‰¹é‡å¤„ç†å®Œæˆ")
        return results
    
    def export_tagged_articles(self, output_file: str) -> None:
        """å¯¼å‡ºå¸¦æ ‡ç­¾çš„æ¡æ¬¾åˆ°JSONæ–‡ä»¶"""
        print(f"æ­£åœ¨å¯¼å‡ºå¸¦æ ‡ç­¾çš„æ¡æ¬¾åˆ°: {output_file}")
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_dir = os.path.dirname(output_file)
        if output_dir and not os.path.exists(output_dir):
            os.makedirs(output_dir)
        
        # å¯¼å‡ºæ•°æ®
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(self.tagged_articles, f, ensure_ascii=False, indent=2)
        
        # è¾“å‡ºå®Œæˆæç¤º
        print("âœ… å¤šç»´åº¦æ ‡ç­¾æ ‡æ³¨å®Œæˆï¼")
        print(f"ğŸ“„ æ ‡æ³¨ç»“æœå·²ä¿å­˜è‡³: {output_file}")
        print(f"ğŸ·ï¸  å…±å¤„ç† {len(self.tagged_articles)} ä¸ªæ¡æ¬¾")
    
    def export_metadata(self, output_file: str) -> None:
        """å¯¼å‡ºå…ƒæ•°æ®åˆ°JSONæ–‡ä»¶"""
        print(f"æ­£åœ¨å¯¼å‡ºå…ƒæ•°æ®åˆ°: {output_file}")
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_dir = os.path.dirname(output_file)
        if output_dir and not os.path.exists(output_dir):
            os.makedirs(output_dir)
        
        # å¯¼å‡ºæ•°æ®
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(self.metadata_store, f, ensure_ascii=False, indent=2)
        
        # è¾“å‡ºå…ƒæ•°æ®å®Œæˆæç¤º
        print(f"ğŸ“Š å…ƒæ•°æ®å·²ä¿å­˜è‡³: {output_file}")
        
        print(f"æˆåŠŸå¯¼å‡º {len(self.metadata_store)} æ¡å…ƒæ•°æ®")
    
    def analyze_tag_distribution(self) -> pd.DataFrame:
        """åˆ†ææ ‡ç­¾åˆ†å¸ƒæƒ…å†µ"""
        print("æ­£åœ¨åˆ†ææ ‡ç­¾åˆ†å¸ƒæƒ…å†µ...")
        
        # åˆ›å»ºæ ‡ç­¾è®¡æ•°å­—å…¸
        tag_counts = {dimension: {} for dimension in self.tag_trees.keys()}
        
        for article in self.tagged_articles:
            article_tags = article.get('tags', {})
            for dimension, tags in article_tags.items():
                for tag in tags:
                    if tag in tag_counts[dimension]:
                        tag_counts[dimension][tag] += 1
                    else:
                        tag_counts[dimension][tag] = 1
        
        # è½¬æ¢ä¸ºDataFrameå¹¶å¯è§†åŒ–
        for dimension, counts in tag_counts.items():
            print(f"\n{dimension}ç»´åº¦æ ‡ç­¾åˆ†å¸ƒ:")
            df = pd.DataFrame(list(counts.items()), columns=['æ ‡ç­¾', 'æ•°é‡'])
            df = df.sort_values('æ•°é‡', ascending=False)
            
            # æ‰“å°å‰10ä¸ªæ ‡ç­¾
            print(df.head(10).to_string(index=False))
            
            # å¯è§†åŒ–
            self._visualize_tag_distribution(df, dimension)
        
        return tag_counts
    
    def _visualize_tag_distribution(self, df: pd.DataFrame, dimension: str) -> None:
        """å¯è§†åŒ–æ ‡ç­¾åˆ†å¸ƒæƒ…å†µ"""
        plt.figure(figsize=(12, 6))
        
        # å¦‚æœæ ‡ç­¾å¤ªå¤šï¼Œåªæ˜¾ç¤ºå‰20ä¸ª
        if len(df) > 20:
            df = df.head(20)
        
        plt.bar(df['æ ‡ç­¾'], df['æ•°é‡'])
        plt.title(f'{dimension}ç»´åº¦æ ‡ç­¾åˆ†å¸ƒ')
        plt.xlabel('æ ‡ç­¾')
        plt.ylabel('æ•°é‡')
        plt.xticks(rotation=45, ha='right')
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        output_file = f'tag_distribution_{dimension}.png'
        output_path = os.path.join(self.script_dir, output_file)
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        print(f"æ ‡ç­¾åˆ†å¸ƒå›¾å·²ä¿å­˜è‡³: {output_path}")
    
    def evaluate_tagging_results(self, ground_truth_file: str = None) -> Dict[str, Dict[str, float]]:
        """è¯„ä¼°æ ‡ç­¾æ ‡æ³¨ç»“æœï¼ˆå¦‚æœæœ‰çœŸå®æ ‡ç­¾æ•°æ®ï¼‰"""
        if not ground_truth_file or not os.path.exists(ground_truth_file):
            print("æœªæä¾›çœŸå®æ ‡ç­¾æ•°æ®ï¼Œæ— æ³•è¿›è¡Œè¯„ä¼°")
            return {}
        
        print(f"æ­£åœ¨è¯„ä¼°æ ‡ç­¾æ ‡æ³¨ç»“æœï¼Œä½¿ç”¨çœŸå®æ ‡ç­¾æ•°æ®: {ground_truth_file}")
        
        # åŠ è½½çœŸå®æ ‡ç­¾æ•°æ®
        with open(ground_truth_file, 'r', encoding='utf-8') as f:
            ground_truth = {article.get('id'): article.get('tags', {}) for article in json.load(f)}
        
        # åˆ›å»ºè¯„ä¼°ç»“æœå­—å…¸
        evaluation_results = {}
        
        # å¯¹æ¯ä¸ªç»´åº¦è¿›è¡Œè¯„ä¼°
        for dimension in self.tag_trees.keys():
            # è·å–æ‰€æœ‰å¯èƒ½çš„æ ‡ç­¾
            all_tags = self._get_all_tags_in_dimension(dimension)
            
            # å‡†å¤‡çœŸå®æ ‡ç­¾å’Œé¢„æµ‹æ ‡ç­¾
            y_true = []
            y_pred = []
            
            for article in self.tagged_articles:
                article_id = article.get('id')
                
                # å¦‚æœæœ‰çœŸå®æ ‡ç­¾
                if article_id in ground_truth:
                    # è½¬æ¢ä¸ºå¤šæ ‡ç­¾æ ¼å¼
                    true_tags = set(ground_truth[article_id].get(dimension, []))
                    pred_tags = set(article.get('tags', {}).get(dimension, []))
                    
                    # å¯¹æ¯ä¸ªæ ‡ç­¾è¿›è¡Œè¯„ä¼°
                    for tag in all_tags:
                        y_true.append(1 if tag in true_tags else 0)
                        y_pred.append(1 if tag in pred_tags else 0)
            
            # è®¡ç®—è¯„ä¼°æŒ‡æ ‡ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”ç”¨ä¸­å¯èƒ½éœ€è¦æ›´å¤æ‚çš„è¯„ä¼°ï¼‰
            if y_true and y_pred:
                # è®¡ç®—å‡†ç¡®ç‡ï¼ˆç®€å•è®¡ç®—ï¼‰
                correct = sum(1 for t, p in zip(y_true, y_pred) if t == p)
                accuracy = correct / len(y_true)
                
                evaluation_results[dimension] = {'accuracy': accuracy}
                print(f"{dimension}ç»´åº¦è¯„ä¼°ç»“æœ: å‡†ç¡®ç‡ = {accuracy:.4f}")
        
        return evaluation_results

# ä¸»å‡½æ•°ç¤ºä¾‹
def main():
    print("ğŸ“‚ æ­£åœ¨åˆå§‹åŒ–ç³»ç»Ÿ...")
    
    # è·å–è„šæœ¬æ‰€åœ¨ç›®å½•
    script_dir = os.path.dirname(os.path.abspath(__file__))
    print(f"ğŸ“ å·¥ä½œç›®å½•: {script_dir}")
    
    # åˆ›å»ºæ ‡ç­¾ç³»ç»Ÿå®ä¾‹
    tagger = LegalMultiDimensionalTagger()
    
    try:
        # ç¤ºä¾‹1ï¼šå¤„ç†å•ä¸ªæ–‡ä»¶ - ä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼ˆsegmented_articles å®é™…æ–‡ä»¶åï¼‰
        input_file = os.path.join(script_dir, "ä¸­åäººæ°‘å…±å’Œå›½æ°‘æ³•å…¸_æ¡æ¬¾åˆ†å‰².json")
        if os.path.exists(input_file):
            print(f"\n=== å¤„ç†å•ä¸ªæ–‡ä»¶: {input_file} ===")
            
            # åŠ è½½æ¡æ¬¾
            tagger.load_articles(input_file, source_institution="ä¸­å›½æ³•å¾‹æ³•è§„æ•°æ®åº“")
            
            # è§„åˆ™æ ‡æ³¨
            tagger.rule_based_tagging()
            
            # åŠç›‘ç£æ ‡æ³¨
            tagger.semi_supervised_tagging()
            
            # å¯¼å‡ºç»“æœ - ä½¿ç”¨ç›¸å¯¹è·¯å¾„
            output_file = os.path.join(script_dir, "tagged_articles.json")
            tagger.export_tagged_articles(output_file)
            
            # å¯¼å‡ºå…ƒæ•°æ® - ä½¿ç”¨ç›¸å¯¹è·¯å¾„
            metadata_file = os.path.join(script_dir, "tagged_metadata.json")
            tagger.export_metadata(metadata_file)
            
            # åˆ†ææ ‡ç­¾åˆ†å¸ƒ
            tagger.analyze_tag_distribution()
            
        # ç¤ºä¾‹2ï¼šæ‰¹é‡å¤„ç†å¤šä¸ªæ–‡ä»¶
        # æŸ¥æ‰¾æ‰€æœ‰æ¡æ¬¾åˆ†å‰²æ–‡ä»¶ - ä½¿ç”¨è„šæœ¬ç›®å½•
        clause_files = [os.path.join(script_dir, f) for f in os.listdir(script_dir) if f.endswith('_æ¡æ¬¾åˆ†å‰².json')]
        if len(clause_files) > 1:
            print(f"\n=== æ‰¹é‡å¤„ç†å¤šä¸ªæ–‡ä»¶: {len(clause_files)} ä¸ª ===")
            
            # å¯¹äºå¤§é‡æ–‡ä»¶ï¼Œä½¿ç”¨å¹¶è¡Œå¤„ç†
            if len(clause_files) > 5:
                tagger.parallel_batch_process(clause_files)
            else:
                tagger.batch_process_articles(clause_files)
            
    except Exception as e:
        print(f"ç¨‹åºè¿è¡Œå‡ºé”™: {str(e)}")

if __name__ == "__main__":
    print("ğŸš€ å¯åŠ¨å¤šç»´åº¦æ ‡ç­¾æ ‡æ³¨ç³»ç»Ÿ...")
    try:
        main()
    except Exception as e:
        print(f"âŒ ç¨‹åºè¿è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()