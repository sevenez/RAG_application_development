{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os                                                                                          # 操作系统接口模块，用于文件和目录操作\n",
    "import time                                                                                       # 时间相关函数，用于性能计时\n",
    "import torch                                                                                      # PyTorch深度学习框架，用于GPU检测\n",
    "import socket                                                                                     # 网络通信模块，用于端口检查\n",
    "from pathlib import Path                                                                          # 面向对象的路径操作模块\n",
    "from dotenv import load_dotenv                                                                    # 环境变量加载模块，用于读取.env文件\n",
    "from fastapi import FastAPI, HTTPException                                                        # FastAPI框架和HTTP异常处理\n",
    "from fastapi.middleware.cors import CORSMiddleware                                                # CORS中间件，处理跨域请求\n",
    "from pydantic import BaseModel                                                                    # Pydantic模型基类，用于数据验证\n",
    "from typing import Dict, Any                                                                      # 类型提示，用于函数签名\n",
    "import uvicorn                                                                                    # ASGI服务器，用于运行FastAPI应用\n",
    "from contextlib import asynccontextmanager                                                        # 异步上下文管理器，用于应用生命周期管理\n",
    "\n",
    "# LangChain相关导入\n",
    "from langchain_huggingface import HuggingFaceEmbeddings                                           # HuggingFace嵌入模型封装\n",
    "from langchain_community.vectorstores import FAISS                                                # FAISS向量数据库实现\n",
    "from langchain_core.prompts import ChatPromptTemplate                                               # 聊天提示模板\n",
    "from langchain_deepseek import ChatDeepSeek                                                       # DeepSeek聊天模型\n",
    "from sentence_transformers import SentenceTransformer as ST                                       # 句子变换器，用于获取设备信息\n",
    "\n",
    "# 全局变量\n",
    "vector_store = None                                                                              # 全局向量存储实例\n",
    "llm = None                                                                                       # 全局语言模型实例\n",
    "prompt = None                                                                                    # 全局提示模板实例\n",
    "init_info = {}                                                                                   # 全局初始化信息字典\n",
    "\n",
    "\n",
    "def is_port_in_use(port: int) -> bool:                                                             # 检查指定端口是否被占用的函数\n",
    "    \"\"\"检查端口是否被占用\"\"\"                                                                       # 函数说明：检查TCP端口是否已被其他进程占用\n",
    "    try:                                                                                            # 异常处理开始\n",
    "        # 创建socket对象\n",
    "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:                               # 创建IPv4 TCP socket对象\n",
    "            # 尝试绑定端口，若成功则端口未被占用\n",
    "            s.bind((\"0.0.0.0\", port))                                                             # 尝试绑定到所有网络接口的指定端口\n",
    "        return False                                                                                # 绑定成功，端口未被占用\n",
    "    except OSError:                                                                                 # 捕获操作系统错误（端口被占用）\n",
    "        return True                                                                                 # 端口已被占用\n",
    "\n",
    "\n",
    "@asynccontextmanager                                                                               # 异步上下文管理器装饰器\n",
    "async def lifespan(app: FastAPI):                                                                   # FastAPI应用生命周期管理函数\n",
    "    global vector_store, llm, prompt, init_info                                                     # 声明使用全局变量\n",
    "\n",
    "    start_total = time.time()                                                                       # 记录初始化总开始时间\n",
    "\n",
    "    load_dotenv()                                                                                   # 加载环境变量配置文件\n",
    "    current_dir = Path(__file__).resolve().parent                                                   # 获取当前脚本所在目录的绝对路径\n",
    "\n",
    "    # 配置路径\n",
    "    LOCAL_EMBEDDING_PATH = current_dir / \"../11_local_models/bge-small-zh-v1.5\"                    # 本地嵌入模型路径配置\n",
    "    FAISS_DB_PATH = current_dir / \"../23-faiss_db\"                                                  # FAISS向量数据库路径配置\n",
    "\n",
    "    # 验证本地嵌入模型是否存在\n",
    "    start_time = time.time()                                                                        # 记录模型验证开始时间\n",
    "    if not LOCAL_EMBEDDING_PATH.exists() or len(os.listdir(LOCAL_EMBEDDING_PATH)) == 0:            # 检查模型目录是否存在且非空\n",
    "        raise FileNotFoundError(f\"本地嵌入模型不存在于路径: {LOCAL_EMBEDDING_PATH}\")                # 模型不存在时抛出异常\n",
    "\n",
    "    print(\"本地嵌入模型已存在，无需下载\")                                                             # 提示模型已存在\n",
    "    model_prep_time = time.time() - start_time                                                      # 计算模型验证耗时\n",
    "    init_info[\"model_prep_time\"] = model_prep_time                                                 # 记录模型验证时间到初始化信息\n",
    "\n",
    "    # 初始化本地嵌入模型\n",
    "    start_time = time.time()                                                                        # 记录嵌入模型初始化开始时间\n",
    "    embeddings = HuggingFaceEmbeddings(                                                             # 创建HuggingFace嵌入模型实例\n",
    "        model=str(LOCAL_EMBEDDING_PATH),                                                             # 指定本地模型路径\n",
    "        model_kwargs={\n",
    "            'device': 'cuda' if torch.cuda.is_available() else 'cpu',                                # 自动选择GPU或CPU设备\n",
    "            'local_files_only': True                                                                  # 强制使用本地文件，不联网下载\n",
    "        },\n",
    "        encode_kwargs={\n",
    "            'batch_size': 32,                                                                         # 批处理大小为32\n",
    "            'device': 'cuda' if torch.cuda.is_available() else 'cpu',                                # 再次指定设备确保一致性\n",
    "            'normalize_embeddings': True                                                              # 启用嵌入向量归一化\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # 获取设备信息\n",
    "    temp_model = ST(str(LOCAL_EMBEDDING_PATH))                                                       # 临时加载模型获取设备信息\n",
    "    device = temp_model.device                                                                       # 获取模型所在设备（GPU/CPU）\n",
    "    del temp_model  # 释放资源                                                                       # 删除临时模型释放内存\n",
    "\n",
    "    embed_time = time.time() - start_time                                                           # 计算嵌入模型初始化耗时\n",
    "    print(f\"嵌入模型初始化耗时：{embed_time:.4f}秒，使用设备：{device}\")                              # 打印初始化耗时和设备信息\n",
    "    init_info[\"embed_time\"] = embed_time                                                            # 记录初始化时间到全局信息\n",
    "    init_info[\"device\"] = str(device)                                                               # 记录设备信息到全局信息\n",
    "\n",
    "    # 加载向量库\n",
    "    start_time = time.time()                                                                        # 记录向量库加载开始时间\n",
    "    if not FAISS_DB_PATH.exists() or len(os.listdir(FAISS_DB_PATH)) == 0:                           # 检查向量库目录是否存在且非空\n",
    "        raise FileNotFoundError(f\"未找到FAISS向量库，路径：{FAISS_DB_PATH}\")                        # 向量库不存在时抛出异常\n",
    "\n",
    "    vector_store = FAISS.load_local(                                                                # 从本地加载FAISS向量库\n",
    "        str(FAISS_DB_PATH),                                                                         # 向量库目录路径\n",
    "        embeddings,                                                                                 # 使用的嵌入模型\n",
    "        allow_dangerous_deserialization=True                                                        # 允许反序列化（注意安全风险）\n",
    "    )\n",
    "    print(f\"加载已存在的FAISS向量库，路径：{FAISS_DB_PATH}\")                                         # 提示向量库加载成功\n",
    "    vector_time = time.time() - start_time                                                          # 计算向量库加载耗时\n",
    "    init_info[\"vector_time\"] = vector_time                                                          # 记录加载时间到全局信息\n",
    "\n",
    "    # 初始化LLM\n",
    "    start_time = time.time()                                                                        # 记录LLM初始化开始时间\n",
    "    api_key = os.getenv(\"DEEPSEEK_API_KEY\") or \"sk-XXXX\"                                            # 获取API密钥（环境变量或默认值）\n",
    "\n",
    "    llm = ChatDeepSeek(                                                                             # 创建DeepSeek聊天模型实例\n",
    "        model=\"deepseek-chat\",                                                                      # 指定使用deepseek-chat模型\n",
    "        temperature=0.0,                                                                            # 设置温度为0，确保确定性输出\n",
    "        max_tokens=2048,                                                                            # 最大输出令牌数限制为2048\n",
    "        api_key=api_key                                                                             # 传入API密钥\n",
    "    )\n",
    "    llm_time = time.time() - start_time                                                             # 计算LLM初始化耗时\n",
    "    print(f\"创建LLM耗时：{llm_time:.4f}秒\")                                                          # 打印LLM初始化耗时\n",
    "    init_info[\"llm_time\"] = llm_time                                                               # 记录初始化时间到全局信息\n",
    "\n",
    "    # 构建提示模板\n",
    "    prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "    基于以下上下文，用中文简洁准确地回答问题。如果上下文中没有相关信息，\n",
    "    请说\"我无法从提供的上下文中找到相关信息\"。\n",
    "    上下文: {context}\n",
    "    问题: {question}\n",
    "    回答:\"\"\"                                                                                         # 回答开始标记\n",
    "                                              )\n",
    "\n",
    "    # 计算总初始化耗时\n",
    "    total_init_time = time.time() - start_total                                                     # 计算整个初始化流程总耗时\n",
    "    print(f\"\\n===== 服务初始化完成（总耗时：{total_init_time:.4f}秒） =====\")                         # 打印初始化完成信息\n",
    "    init_info[\"total_init_time\"] = total_init_time                                                  # 记录总初始化时间到全局信息\n",
    "\n",
    "    yield  # 应用运行期间保持资源                                                                    # 应用启动完成，开始处理请求\n",
    "\n",
    "\n",
    "# 初始化FastAPI应用\n",
    "app = FastAPI(                                                                                      # 创建FastAPI应用实例\n",
    "    title=\"法律文档问答服务\",                                                                        # 设置API文档标题\n",
    "    description=\"基于本地文档的法律问答API\",                                                           # 设置API文档描述\n",
    "    lifespan=lifespan                                                                              # 指定生命周期管理函数\n",
    ")\n",
    "\n",
    "# 配置CORS\n",
    "app.add_middleware(                                                                               # 添加CORS中间件\n",
    "    CORSMiddleware,                                                                                # CORS中间件类\n",
    "    allow_origins=[\"*\"],                                                                           # 允许所有来源的跨域请求\n",
    "    allow_credentials=True,                                                                        # 允许携带凭证\n",
    "    allow_methods=[\"*\"],                                                                           # 允许所有HTTP方法\n",
    "    allow_headers=[\"*\"],                                                                           # 允许所有请求头\n",
    ")\n",
    "\n",
    "\n",
    "# 定义请求和响应模型\n",
    "class QueryRequest(BaseModel):                                                                      # 查询请求数据模型\n",
    "    question: str                                                                                  # 用户问题字段\n",
    "\n",
    "\n",
    "class QueryResponse(BaseModel):                                                                     # 查询响应数据模型\n",
    "    question: str                                                                                  # 原始问题\n",
    "    answer: str                                                                                    # 生成的答案\n",
    "    retrieval_time: float                                                                          # 文档检索耗时（秒）\n",
    "    llm_time: float                                                                                # LLM生成耗时（秒）\n",
    "    total_time: float                                                                              # 总处理耗时（秒）\n",
    "    retrieved_docs_count: int                                                                      # 检索到的相关文档数量\n",
    "\n",
    "\n",
    "# 健康检查接口\n",
    "@app.get(\"/health\", response_model=Dict[str, str])                                                # 健康检查GET接口\n",
    "async def health_check():                                                                           # 健康检查处理函数\n",
    "    return {\"status\": \"healthy\", \"message\": \"法律文档问答服务运行正常\"}                            # 返回健康状态和服务信息\n",
    "\n",
    "\n",
    "# 获取初始化信息接口\n",
    "@app.get(\"/init-info\", response_model=Dict[str, Any])                                              # 初始化信息GET接口\n",
    "async def get_init_info():                                                                          # 初始化信息处理函数\n",
    "    return init_info                                                                                # 返回全局初始化信息\n",
    "\n",
    "\n",
    "# 问答接口\n",
    "@app.post(\"/query\", response_model=QueryResponse)                                                 # 问答POST接口\n",
    "async def query(request: QueryRequest):                                                             # 问答处理函数\n",
    "    question = request.question.strip()                                                             # 获取并清理用户问题\n",
    "    print(f\"\\n收到客户端查询: {question}\")                                                          # 打印收到的查询\n",
    "\n",
    "    if not question:                                                                                # 检查问题是否为空\n",
    "        raise HTTPException(status_code=400, detail=\"问题不能为空\")                               # 空问题返回400错误\n",
    "\n",
    "    query_start = time.time()                                                                       # 记录查询处理开始时间\n",
    "\n",
    "    try:                                                                                            # 异常处理开始\n",
    "        # 检索相关文档\n",
    "        start_time = time.time()                                                                    # 记录检索开始时间\n",
    "        retrieved_docs = vector_store.similarity_search(question, k=3)                             # 执行相似度搜索，返回前3个相关文档\n",
    "        retrieve_time = time.time() - start_time                                                    # 计算检索耗时\n",
    "        print(f\"文档检索耗时：{retrieve_time:.4f}秒，返回{len(retrieved_docs)}条相关结果\")          # 打印检索结果\n",
    "\n",
    "        # 准备上下文内容\n",
    "        docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)                     # 将检索到的文档内容合并为上下文\n",
    "\n",
    "        # 调用LLM生成答案\n",
    "        start_time = time.time()                                                                    # 记录LLM调用开始时间\n",
    "        answer = llm.invoke(prompt.format(question=question, context=docs_content))                 # 格式化提示并调用LLM\n",
    "        llm_time = time.time() - start_time                                                         # 计算LLM调用耗时\n",
    "        print(f\"答案生成耗时：{llm_time:.4f}秒\")                                                     # 打印LLM调用耗时\n",
    "\n",
    "        # 计算总耗时\n",
    "        query_total_time = time.time() - query_start                                                # 计算整个查询处理耗时\n",
    "\n",
    "        return QueryResponse(                                                                        # 返回结构化的响应数据\n",
    "            question=question,                                                                       # 原始问题\n",
    "            answer=answer.content,                                                                   # 生成的答案内容\n",
    "            retrieval_time=retrieve_time,                                                          # 检索耗时\n",
    "            llm_time=llm_time,                                                                       # LLM调用耗时\n",
    "            total_time=query_total_time,                                                             # 总处理耗时\n",
    "            retrieved_docs_count=len(retrieved_docs)                                                 # 检索到的文档数量\n",
    "        )\n",
    "    except Exception as e:                                                                          # 捕获所有异常\n",
    "        print(f\"处理查询时发生错误：{str(e)}\")                                                       # 打印错误信息\n",
    "        raise HTTPException(status_code=500, detail=f\"处理查询时发生错误：{str(e)}\")                  # 返回500服务器错误\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":                                                                          # 主程序入口\n",
    "    # 配置服务端口\n",
    "    PORT = 8848                                                                                     # 设置服务监听端口为8848\n",
    "\n",
    "    # 检查端口是否被占用\n",
    "    if is_port_in_use(PORT):                                                                        # 检查8848端口是否已被占用\n",
    "        # 端口被占用时抛出明确错误\n",
    "        raise OSError(f\"端口 {PORT} 已被占用，请释放该端口或更换其他端口后重试。\")                      # 端口占用异常提示\n",
    "\n",
    "    # 端口可用时启动服务\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=PORT, log_level=\"info\")                                  # 启动Uvicorn服务器，监听所有网络接口"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
